{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fecdce2-2377-4c84-a245-9af20f76e634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc042a58-d12d-462d-baf6-80646ba6cdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a23627-418b-4371-84ab-7d0904d8e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/17/2024-21:02:33] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(TRT_LOGGER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bda59b-1ae4-4c9f-9242-b2e41fdf0442",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = trt.Runtime(TRT_LOGGER)\n",
    "with open('./engine.trt', 'rb') as f:\n",
    "    engine_bytes = f.read()\n",
    "    engine = runtime.deserialize_cuda_engine(engine_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f05b83-0dcd-4811-aa6e-aa7e9fbd48e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/17/2024-21:02:39] [TRT] [W] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n"
     ]
    }
   ],
   "source": [
    "# Create execution context as shown below\n",
    "bert_context = engine.create_execution_context()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d449d240-253d-4064-9aa1-a549d69f68a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  1996  3007  1997  2605  1010   103  1010  3397  1996  1041 13355\n",
      "   2884  3578  1012   102]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "inputs\n",
    "text = \"The capital of France, \" + tokenizer.mask_token + \", contains the Eiffel Tower.\"\n",
    "    # input_ids are the indices corresponding to each token in the sentence.\n",
    "    # attention_mask indicates whether a token should be attended to or not.\n",
    "    # token_type_ids identifies which sequence a token belongs to when there is more than one sequence.\n",
    "'''\n",
    "# input_ids = numpy array ( size: batch X seq_len) ex: (1 X 30 )\n",
    "# token_type_ids = numpy array ( size: batch X seq_len) ex: (1 X 30 )\n",
    "# attention_mask = numpy array ( size: batch X seq_len) ex: (1 X 30 )\n",
    "BERT_PATH = 'bert-base-uncased'\n",
    "npz_file = BERT_PATH + '/case_data.npz'\n",
    "data = np.load(npz_file)\n",
    "input_ids = data['input_ids']\n",
    "token_type_ids = data['token_type_ids']\n",
    "position_ids = data['position_ids']\n",
    "attention_mask = data['attention_mask']\n",
    "print(data['input_ids'])\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n",
    "\n",
    "'''\n",
    "outputs\n",
    "'''\n",
    "bert_output = torch.zeros((1, 16),device=device).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b318a29-6b0b-4ae6-a718-0ffbd641bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate memory for the inputs and outputs in GPU\n",
    "batch_size = 1\n",
    "\n",
    "'''\n",
    "memory allocation for inputs\n",
    "'''\n",
    "d_input_ids = cuda.mem_alloc(batch_size * input_ids.nbytes)\n",
    "d_token_type_ids = cuda.mem_alloc(batch_size * token_type_ids.nbytes)\n",
    "d_attention_mask = cuda.mem_alloc(batch_size * attention_mask.nbytes)\n",
    "\n",
    "'''\n",
    "memory allocation for outputs\n",
    "'''\n",
    "d_output = cuda.mem_alloc(batch_size * bert_output.nbytes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2bd0f98-af5f-4dbe-8312-3c5215ed67f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02/17/2024-21:19:35] [TRT] [W] The enqueue() method has been deprecated when used with engines built from a network created with NetworkDefinitionCreationFlag::kEXPLICIT_BATCH flag. Please use enqueueV2() instead.\n",
      "[02/17/2024-21:19:35] [TRT] [W] Also, the batchSize argument passed into this function has no effect on changing the input shapes. Please use setBindingDimensions() function to change input shapes instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7363/2633436634.py:11: DeprecationWarning: Use execute_async_v2 instead.\n",
      "  bert_context.execute_async(batch_size, bindings, stream.handle, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create bindings array\n",
    "bindings = [int(d_input_ids), int(d_token_type_ids), int(d_attention_mask), int(d_output)]\n",
    "\n",
    "# Create stream and transfer inputs to GPU (can be sync or async ). ‘async ’ shown here.\n",
    "stream = cuda.Stream()# Transfer input data from python buffers to device(GPU)\n",
    "cuda.memcpy_htod_async(d_input_ids, input_ids, stream)\n",
    "cuda.memcpy_htod_async(d_token_type_ids, token_type_ids, stream)\n",
    "cuda.memcpy_htod_async(d_attention_mask, attention_mask, stream)\n",
    "\n",
    "# Execute using the engine\n",
    "bert_context.execute_async(batch_size, bindings, stream.handle, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21bbabe7-413f-4e44-b744-58e01f8dbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer output back from GPU to python buffer variable\n",
    "cuda.memcpy_dtoh_async(bert_output, d_output, stream)\n",
    "stream.synchronize()\n",
    "\n",
    "# Now the bert_output variable in which we stored zeros will have the prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6b47b82-2ba6-434c-93f1-9f6718cd76f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m pred_output_softmax \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m mask_index \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(input_ids[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mmask_token_id)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m mask_word \u001b[38;5;241m=\u001b[39m \u001b[43mpred_output_softmax\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m top_10 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(mask_word, \u001b[38;5;241m10\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel test topk10 output:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Run softmax and get the most probable class\n",
    "pred = torch.tensor(bert_output)\n",
    "pred_output_softmax = F.softmax(pred, dim=-1)\n",
    "mask_index = np.where(input_ids[0] == tokenizer.mask_token_id)[0][0]\n",
    "mask_word = pred_output_softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim=1)[1][0]\n",
    "print(\"model test topk10 output:\")\n",
    "for token in top_10:\n",
    "    word = tokenizer.decode([token])\n",
    "    new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "    print(new_sentence)\n",
    "_, predicted = torch.max(pred_output_softmax, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b83a1957-5312-482d-ae4e-fed312f62f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
       "         0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_output_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bf0badd-6f89-448d-bfa6-2c761279fe8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a97a99-f46a-4855-8bb8-75cecf534dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
